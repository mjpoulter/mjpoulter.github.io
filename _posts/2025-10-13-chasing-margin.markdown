---
layout: post
title:  "Chasing Margins"
date:   2025-10-13
categories: jekyll update
---

Before going to this weeks main topic i would recommend a great conversation on the a16z podcast with Vishal Misra which does a great job of explaining conceptually how LLMs work and why they wont get to AGI without a change of architecture. Confirmation bias for me perhaps but good listen <https://www.youtube.com/watch?v=uRuY0ozEm3Q>

Crazy week with more big $$ deals announced by Openai, hedging their bets and brining in a competitor to Nvidia in a deal with AMD and then in concert with my current thesis (folks starting to need to figure out profitabiity) a deal with Broadcom to co-create their own chips (for inference) with an assist from ARM (Softbank incented). The hype cycle continues and more and more folks are starting to be concerned about when rather than if the bubble pops and a key part of the concern is both the revenue and margin problem. Who's going to pay, how much are they willing to pay, and what are the costs to deliver that revenue and hence overall margins.

Snippets that play into this story are that AWS now delivers half of its bedrock compute via trainium, microsoft continue to pursue their own chips and google continues to advance tpu's and perhaps even sell to outside customers(meta also still working on their own chips) while also having to work with Nvidia (and in some cases AMD) to serve either customer requirments or current supply. These are all optionality and margin bets from the hyperscalers. Similarly if you go upstack Openai's move is both optionality/control but also margin management.

To date the winner in this AI cycle has clearly been Nvidia with some value accruing to google and maybe some to microsoft and a little to aws. The neocloud and datacenter companies are all pouring in capital but totally dependent on Nvidia currently for both revenue and margin. Many are starting to diversify as seen by coreweaves acquisition of software companies to try to improve margins and be more than infrasturcture providers. Its not clear that any of the frontier labs can be profitable with the current economics, though they are starting to get some sticky revenue in markets like coding, and by having their own products anthropic (claude code) and openai (GPT-5 codex) and google (gemini codeassist/cli) are taking revenue from the upstream products like cursor, replit, even github copilot, ie they are competing with their own customers. They in turn are starting to look at building their own models (latest rumor around cursor) especially for simple activities like code completion, in order to reduce their payments to the model suppliers, and in some cases moving to hosting open models themselves. The inference hosting companies (eg together, fireworks etc) are also now moving to hosting their own compute vs renting from the hyperscalers or neoclouds again all about margin and control.

My own view is that the most value will continue to accrue to the chip providers. Its going to be hard to keep up with Nvidia in the short term (even their contribution of the vera rubin datacenter infra to opencompute is enlightened self interest). But as the cloud providers have shown you can improve your margins by vertical integration, aws in particular has a long track record of doing this after their anapurna acquistion (of course apple has been doing that for a while with apple silicon), and google with tpu's, but they were optimizing profitable businesses and giving themselves options and control in the face of intel and/or amd and/or qualcomm and/or broadcom and/or cisco putting up prices. I think those guys have the scale to continue to keep on doing that. Its hard to see how the neoclouds who have big dependencies on Nvidia at the moment could do that and i think the inference providers will probably find that they dont improve their lot by trying to go full stack at their scale. Its a path many saas providers moved away from as the undifferentiated heavy lifting of infrastructure is probably best left to others. Its harder than it looks. I expect that the model providers, other than openai, will stick to working with hyperscalers, and help them optimize their chips and/or tailor their models/training to a range of chips, eg tranium, tpu, beside nvidia, vs doing it themselves.

Openai seems to think that it needs to be totally vertically integrated for its long term success. Its hard to see that playing out well for them given the scale of investment required and their current margins. At some point either they charge for their product vs having most users be on chatgpt for free or they find a way to monetize that userbase some other way (advertising, passthru tax). Maybe they cover some of their costs with industry specific solutions, like coding, but coding is reasonably horizontal and a discipline they understand, not clear other industries are as easy to master, maybe search. Maybe applications built on top of them are willing to pay more to cover costs, but thats currently not clear and the competition is still there in anthropic and the open models. The model providers, are really the OS providers of this space and so much like microsoft or apple (or redhat) you'd think they should be able to capture margin, but so far that isnt happening and yet unlike their predecessors they are pushing a rapid build out with high costs and low margins. Maybe they are following the jeff bezos playbook from the early days of amazon's buildout, but that was a clearer path than the one being taken by openai currently and they have commented that more deals are to come.

Anyway currently i dont see anyone but nvidia making money. Everyone else is chasing market share and now trying to figure out how to be profitable and as (in most cases) they dont seem to be able to put up prices sufficiently to cover costs they are looking at reducing their expenses. Im just not convinced by the approaches. Maybe eventually the consumer apps just go to advertising, which is the most likely play, but you cant do that for enterprise applications and while enteprises have got used to usage based pricing for cloud usage, the whole finops discipline is painful and they'd prefer more predictable costs, and subscription based pricing, but that price has to feel appropriate for the value realized and as microsoft is finding with copilot, most enterprises arent buying that equation yet, which is one reaon why microsoft is working to drop their costs of delivery also.

Im not sure how this all plays out, some mix of advertising, subscriptions, revenue sharing for consumer apps, and clearer value based pricing for enterprises. Hard to see any of that covering the costs implied for the current buildout, which seems over the top to me. But at least folks are starting to make the moves that indicate they now see the need to illustrate a path to profitability. Otherwise im guessing the funding dries up before they get there.

i guess we'll find out.
{% include comments.html %}
