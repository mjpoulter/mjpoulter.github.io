---
layout: post
title:  "Investment Diversity - Alternatives to LLMs for reasoning and generalizable intelligence"
date:   2025-09-02
categories: jekyll update
---
Following on from last week and my thoughts on the need for more efficient approaches to AI infrastructure, particularly for inference, i was thinking about better uses for the capital that is inflating the current investment bubble. That brought me to considering alternatives to large language models (LLMs) for reasoning and generalizable intelligence tasks. 

I think im in the (growing) sceptic camp around the potential of current architectures to get to human-level reasoning and generalization. The focus on scaling LLMs seems to overlook the need for more diverse approaches that could lead to breakthroughs in AI capabilities. There are prominent sceptic/critics like Gary Marcus and more recently Yann LeCun who have proposed alternative approaches and recent research that confirms (to me intuitive) result that LLMs dont reason, but produce results that are associative from their training data. <https://garymarcus.substack.com/p/a-knockout-blow-for-llms>
,<https://machinelearning.apple.com/research/illusion-of-thinking>

That isnt to say that LLMs dont have powerful capabilities, they clearly do, but mostly limited to generative capabilties around language and everything is somewhere intheir training data and its associations. And there are domains, such as coding, that can benefit enormously from these capabilities, however even in the broader domain of software engineering and systems development they start to show their limitiations and need augmenting with strategies such as RAG and MCP, which Mr Marcus would probably suggest is a form or neurosymbolic AI for context and control/orchestration flows to mimic reasoning.

The good news that we do have some folks pursuing alternatives, Google Deepmind tends to take an approach of specialized models for each domain (AlphaXXX) and there is a growing interest in World Models, i think driven by the needs of robotics and autonomous systems such as self-driving cars. <https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/> , <https://www.nvidia.com/en-us/glossary/world-models/> and you see important AI figures such as Fei Fei Li at WorldLabs and Yann LeCun with the JEPA approach at MetaAI actively pursing research in the field, with a small number of startups. Recently there has also been some work on alternative reasonsing approaches eg. <https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples> , <https://github.com/sapientinc/HRM> and you hope the researchers at Apple are working on something given they just debunked the LLMs can reason mindset. The question is can these approaches get funding, given the LLMs can scale to AGI noise/hype, and is there enough diversity of approach and willingness to combine approaches given that folks who have put in large amounts of capital will start wanting a return, and of course if the bubble bursts does that kill all these nascent research efforts. Im optimistic that the Deepmind folks at least will continue and that there is enough interest in robotic autonomy that the push will continue, but it would certaintly be nice if some of that capital chasing datacenter build outs could be redirected towards these alternative approaches.

I think all of these approaches will need to be combined in some way to get to true generalizable intelligence. Again using the example of my granddaughter, she learns through all her senses. I dont agree with LeCun's mindset that most of it comes from the visual understanding, hence I-JEPA and V-JEPA. From what i see language is a key part of reinforcement, she listens, she understands thru words and it reinforces her real world experiences, even or especially when the word is "No!", and when she anticipates the next page in a book, her world model is clearly at work. I think we need this sort of combined approach to get close to human-level behavior. Perhaps its higher level coordination of many different sub-models with their own specializations integrated with a world model. But it doesnt feel like a single approach will get us there if we consider the way nature has found its way to intelligence.

Back to my own area of expertise in software, systems architecture and development, i think we need these alternative reasoning approaches and "world" models of systems to really get full leverage from AI. We'll certainly continue to get more leverage from tuning what is currently available and under development, but it feels like we are leaving potential benefits on the table, and i think we need a longer term approach and a "long term greedy" vs "short term greedy" perspective on the investment. Ie someone saying its going to take 5-10 years, vs AGI (or self-driving cars, or autonomous humanoid robots) will be here next week. But thats unlikley to be a winning pitch for funding. We'll see, maybe as more of the luminaries admit that emperor isnt wearing clothes we'll get a correction, before the bubble bursts or winter sets in (again). Of course, time will tell.

{% include comments.html %}