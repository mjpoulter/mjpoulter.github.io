---
layout: post
title:  "The need for more efficient AI infrastructure"
date:   2025-08-26
categories: jekyll update
---
This week ive been thinking about infrastructure and bubbles. Specifically the efficiency of current AI infrastructure and the push to build out massive amounts of compute, with the attendant amount of power and cooling to support, in pursuit of delivering more and more capacity to support training and inference of leading edge AI models. Folks continue to say they are compute limited, the OpenAI CFO remarking on that last week, even while the CEO says that we are probably in a bubble. Having worked in a compute limited environment (seismic processing) i understand that scenario, but we always worried about the cost, which is seemingly not a factor folks are concerned about here, being convinced that massive investments and losses now are needed to get to future capabilities (and profits)? Feels a lot like 2001 and all the server, router, fiber build out of the internet boom/bust, which we eventually got to learn from and leverage. Coincidentally its Hotchips 2025 which i always enjoy following, and of course a big AI focus and Nvidia’s results coming Wednesday.

As i was mulling all this i got to see my granddaughter move from crawling to a few steps, to walking over the course of a week and was nudged by nvidia releasing its 130W robotics focused chip(jetson AGX Thor) to compare and contrast the infants 15W human brain self, (adult brains are around 20W) learning a difficult task relative to progress in humanoid robots.  Biology and real intelligence is still clearly ahead when it comes to packaging and power efficiency, and its not a von-neuman architecture as far as i know. Much more for us to learn there which further drove my scepticsm of our current approaches and some thinking about alternatives. Just feels like we are currently brute forcing things and a more subtle, efficient approach awaits discovery, though doing more of the same at scale is clearly less risky than looking for new algorithms and novel hardware. But as i would expect the real value of AI to come from inference at the edge it feels like somethings got to give. Maybe Nvidia has something they are working on, but looking at their robotics and auto efforts they are currently variations on the gpu theme. In this vein, google released an evaluation of the cost of inference in their cloud implementations. The sound bite is that a query is the equivalent of a 9s video, but there’s lots of caveats behind that, but at least they understand its important. <https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference>

HotChips interestingly isnt showing anything particularly novel. Some interesting evolutionary developments, but nothing particularly unexpected, and some software associated. Eg Flash Attention 4 announced, tuned for Blackwell. Of course the cycle on chips is far longer wavelength than software and so its hard to react quickly to software needs which is why most hardware solutions are general purpose and evolutionary rather than application specific, and then in a virtuous cycle the software is optimized for the hardware, which currently deepens Nvidia’s moat, all of which makes google’s TPU investments interesting as a counter to Nvidia’s more general purpose processors, though other competitors aren’t really getting traction relative to processors dedicated to inference. Eg groq

Continuing that thread the week also saw some interesting infrastructure news, focused on efficiency and AI costs.. Eg crusoe buying Atero, and the previously mentioned AI power consumption study from google, and the one that most caught my eye was Meta signing a big cloud computing deal with GCP, which given how Meta has always built their own infrastructure was an interesting move. Perhaps expense vs capital, parallel execution giving more speed, or access to TPU’s vs Nvidia vs their own custom chip efforts driving that decision.

As i mentioned, the Trillions of $$$s going into all this infrastructure build out at this stage of a technologies development certainly reminds me of the late 90’s, early 2000’s internet bubble and all the sun servers, cisco routers and dark fiber build out that was left behind when it burst. Are we going to have zombie datacenters and bucket loads of underutilized Nvidia chips if the current bubble bursts. Feels like the power infrastructure build out will still be valuable, much like fiber in the prior cycle, we’ll see about the chips/servers, they age out pretty quickly.

Going back to my comparison to biology, it also struck me that the build out as currently postulated is an extrapolation from the capabilities and needs of the current technologies and in all likelihood that is a false investment premise. We are probably too early to scale out. To be really cost effective and provide use case value, there needs to be much more efficient, effective implementations of inference if we are going to get it to be mobile phone pervasive. Much like with mobile the incumbents (eg intel) didnt see the market and didnt have the technology that was needed and so newcomers took that market. Much like Intel themselves took the niche that had been occupied previously by IBM, DEC, Sun etc. It feels like Nvidia with Jensen are like Intel with Andy Grove, so maybe Nvidia continue to dominate through this current cycle. And here the market opportunity is obvious and you’d therefore think the impetus to bridge Nvidia’s moat, so far we arent see it, and therefore clusters and superclusters of GPUs are being built out.

From an innovation perspective the comparison to biology had me thinking about neuromorphic  computing as an approach to more efficient inference. <https://www.nature.com/articles/s41467-025-57352-1> Only IBM and Intel have chips in that space and only really experimental in both cases, with IBM likely continuing the research and maybe Intel wont with their other problems. <https://research.ibm.com/blog/northpole-llm-inference-results> , <https://www.intel.com/content/www/us/en/research/neuromorphic-computing-loihi-2-technology-brief.html> 
Still it feels like some approach other than software and power optimizing GPU’s is needed. Perhaps Nvidia has something up their sleeves there. But their current approach is to emulate neuromorphic approaches on  GPU’s. Probably hard for them to see a different approach, similarly ARM, usual issue with not having the wherewithal to disintermediate themselves or the technical mindset to look for alternatives. I might have it wrong but extropolating from previous cycles, if the future of inference is at the edge, in glasses, headsets, phones, tablets, desktops, cars, robots, etc.. and not so much in massive terrawatt sized data centers. The need and the disintermediation will comes from below driven  by a mass market use case,pc’s, phones being examples. Maybe its a software innovation (i personally think that good as they are transformers will dead end in their current form and something new will be needed for true emulation of intelligence with self-learning) or maybe its hardware or a combination of the two.. But the current approach feels unsustainable and all the $$$s being raised pursuing it, feel like they are going to be vaporized. And unless the costs drop and the latency of advanced capabilities improves the apps that leverage the models will find it hard to breakout and gain meaningful margins and grow. Maybe optical computing will come to the fore, there are lots of startups in the optical space, mostly around comms, but some also looking at compute. eg optical matmul. Its a safe space to pursue given the benefits you get from going optical from a power and speed perspective(Geometry are a fund investing in that space) and its already something Nvidia are pursuing. <https://www.tomshardware.com/networking/nvidia-outlines-plans-for-using-light-for-communication-between-ai-gpus-by-2026-silicon-photonics-and-co-packaged-optics-may-become-mandatory-for-next-gen-ai-data-centers>

And from an interview with the Information last week the Lux capital folks think there is room for other approaches to inference, they mentioned FPGAs and ASICs, but many believe it will be really hard to dislodge Nvidia, which may well be true at this stage of the cycle, but who knows how that pans out later. Looking to the past once AMD introduced 64bit intel compatible processors and Intel followed they became the dominant players and  SPARC and all the other server RISC chips started their downward spiral. Ironic of course that the ultimate RISC design ARM then captured mobile by fulfilling the power envelope promise that comes from RISC simplicity.

There are likely other approaches eg Compute in memory? Perhaps memristors finally fulfil their promise, or perhaps compute with flash memory or neuroVSA. Maybe someone finally figures out how the brain works. Just feels like we needs some of the $$$’s currently attached to big build outs looking at alternatives.

Of course, I might well be completely off base, maybe scaleout and evolve is the way to go, because while i think ive seen this movie before, as the saying goes, the past is not a predictor of the future!!

ps . Part of the weeks news cycle has been around Intel and the Softbank and US govt investments, all of which feels more like the death knell than a promising sign. Its not clear to me that Intel can ever recapture its position, given its missed the last 2 technology waves, and the government investment is presumably targeted at ensuring leading edge semiconductor manufacturing, but who is going to be a customer unless Intel’s processes get back to comparability with TSMC. Hard to see Nvidia moving to Intel (Nvidia and TSMC are tied at the hip currently) or others moving from Samsung. Feels like good money after bad, but hopefully im wrong, because unfettered access to leading edge semiconductor manufacturing is clearly a national security issue.

{% include comments.html %}